{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Beginner Exercises- Titanic Dataset","version":"0.3.2","provenance":[{"file_id":"1KRlLtqCXpScrOcpwOg22rmDiz287MbdS","timestamp":1558545497337},{"file_id":"https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_text_classification.ipynb","timestamp":1558536955015}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ItXfxkxvosLH"},"source":["#Beginner Exercises- Titanic Dataset\n","\n","Sources: [Boosted Tress Tf 1.x](https://www.tensorflow.org/tutorials/estimators/boosted_trees_model_understanding#how_to_interpret_boosted_trees_models_both_locally_and_globally),\n","[Boosted Trees Tf 2.0](https://medium.com/tensorflow/how-to-train-boosted-trees-models-in-tensorflow-ca8466a53127)\n","\n","Dataset Information: https://www.kaggle.com/c/titanic/data\n","\n","\n","##Problem Description: \n","\n","The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n","\n","One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n","\n","In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Eg62Pmz3o83v"},"source":["Let's start with importing our libraries: \n","\n","\n","*   TensorFlow\n","*  Pandas\n","* Numpy\n","* Matplotlib pyplot\n","* Seaborn\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qqb0PzPs0Imt","colab":{}},"source":["!pip install tensorflow==2.0.0-alpha0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2ew7HTbPpCJH","colab":{}},"source":["from __future__ import absolute_import, division, print_function\n","\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","print(\"Version: \", tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iAsKG535pHep"},"source":["## Download the Titanic Dataset\n","\n","The Titanic dataset is available on [TensorFlow datasets](https://www.tensorflow.org/datasets/datasets#titanic). For this example we're going to load it directly into a Pandas Dataframe.\n","\n","As we'll see, the team at google has already gone through and cleaned the dataset for us. In future projects, we'll manage the data cleaning.\n","\n","Link to the Datasets:\n","\n","* Training: https://storage.googleapis.com/tf-datasets/titanic/train.csv\n","* Evaluation: https://storage.googleapis.com/tf-datasets/titanic/eval.csv\n","\n","Note that we'll still need to split these into features and labels"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zXXx5Oc3pOmN","colab":{}},"source":["train_feat = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\n","eval_feat = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')\n","train_labels = train_feat.pop('survived')\n","eval_labels = eval_feat.pop('survived')\n","\n","print('Lenght of training set: {}'.format(len(train_feat)))\n","print('Lenght of evaluation set: {}'.format(len(eval_feat)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ABjZzyHHsXHe","colab_type":"text"},"source":["## Explore the data \n","\n","Now let's examine our features and labels.\n","\n","Note that the names, tickets, and cabin data have been removed from our dataset. For the purposes of this example, we will not use that data, however that data can be useful for different solutions to this problem. \n","\n","Definitions: \n","* n_siblings_spouses: Number of siblings/ spouses aboard.\n","* parch: Number of parents / children aboard.\n","* embark_town: what town they left from.\n","* alone: traveling with no siblings, spouses, parents, or children. \n","* Survival: 0=No, 1=Yes"]},{"cell_type":"code","metadata":{"id":"6UCRLXxXmnST","colab_type":"code","colab":{}},"source":["train_feat.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKfZm1qet1p1","colab_type":"code","colab":{}},"source":["train_labels.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kxF5znrvNYGZ","colab_type":"text"},"source":["###Graphs\n","\n","To dive deeper into our datasets, we'll use our graphing libraries to visualize what happened  on the titanic. \n","\n","For starters, let's merge the datasets so we can look at everything."]},{"cell_type":"code","metadata":{"id":"kBgFzmbiOIq-","colab_type":"code","colab":{}},"source":["features = pd.concat([train_feat, eval_feat])\n","labels = pd.concat([train_labels, eval_labels])\n","features['Survived'] = labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ET2f7HYxToxz","colab_type":"text"},"source":["First, let's see exactly how many people survived and how many didn't. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"y8qCnve_-lkO","colab":{}},"source":["fig = plt.figure(figsize=(14,6))      \n","labels.value_counts().plot(kind=\"bar\")  \n","\n","print(\"Survivers: {}\".format(labels.value_counts()[1]))\n","print(\"Didn't make it: {}\".format(labels.value_counts()[0]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RRYiVoZaUBRU","colab_type":"text"},"source":["Pretty tragic turn out.\n","\n","Let's investigate some of the different features and see if there is any correlation between it and the survival. Start with the survival rate by class"]},{"cell_type":"code","metadata":{"id":"DMmfBFKoUV65","colab_type":"code","colab":{}},"source":["sns.factorplot('Survived',data=features,kind='count',hue='class')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kgpcmUHyWZdp","colab_type":"text"},"source":["You'll notice here that third class passengers where far more likely to not have survived. This might be because of where third class is located on the ship. Third class is usually toward the bottom. That makes since why first-class passengers were more likely to survive because their cabins are often on the upper decks of a boat. \n","\n","Next let's look at age to see if there is any correlation\n"]},{"cell_type":"code","metadata":{"id":"iK8KvxX1WY-g","colab_type":"code","colab":{}},"source":["sns.catplot(x='Survived', y='age',data=features)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aZjT0okEYw8N","colab_type":"text"},"source":["From age we only get a sense that those 65+ are likely to not have survived. For every other age you have about a 50% chance of survival.\n","\n","(Interested in that really old guy? Create a new cell and try running this code: `features.loc[features['age'].idxmax()]`)\n","\n","Next let's see if sex has any correlation to survival"]},{"cell_type":"code","metadata":{"id":"8bbOXCTjZmIA","colab_type":"code","colab":{}},"source":["sns.factorplot('Survived',data=features,kind='count',hue='sex')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79NRJVzYZs7_","colab_type":"text"},"source":["Wow, there's something! Men are far more likely to not have survived compared to women. \n","\n","Next let's look at survival rate based on the deck.\n"]},{"cell_type":"code","metadata":{"id":"ctxo7xqBZ6tZ","colab_type":"code","colab":{}},"source":["sns.factorplot('Survived',data=features,kind='count',hue='deck')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aq9Gtj-KbSbZ","colab_type":"text"},"source":["What you'll notice is that we have too many unknown decks. Let's deal with these unknown decks and see if we gain any insights. "]},{"cell_type":"code","metadata":{"id":"ap6rumyybYSx","colab_type":"code","colab":{}},"source":["sns.factorplot('Survived',data=features,kind='count',hue='deck',hue_order=['C', 'G', 'A', 'B', 'D', 'F', 'E'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAQCbaPOjIWm","colab_type":"text"},"source":["Now this is a little more interesting. Those on decks B, D, and E were more likely to survive. The other decks are around a 50/50 chance. Important to note that the majority of passengers had unknown decks, so this data could be skewed. "]},{"cell_type":"markdown","metadata":{"id":"QasmlWhipqXP","colab_type":"text"},"source":["## Build the model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LLC02j2g-llC"},"source":["###Feature Columns\n","\n","We've had fun exploring the data and finding correlations. Now let's build a model that can do the predictions for us. \n","\n","Our first  step in building the model is to prepare our feature columns. Our model will use [Boosted Trees](https://medium.com/tensorflow/how-to-train-boosted-trees-models-in-tensorflow-ca8466a53127) through the [tf.estimators](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator) api. Boosted Trees uses an ensemble of decision trees to make better predictions. \n","\n","To work inside of [tf.estimator.BoostedTreesClassifier](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/BoostedTreesClassifier) we will need to transform our features into [tf.feature_column](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column) so that they may be used inside TensorFlow. We will need to create a Feature list with our categorical columns and numeric columns\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_NUbzVeYkgcO","colab":{}},"source":["fc = tf.feature_column\n","CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck', \n","                       'embark_town', 'alone']\n","NUMERIC_COLUMNS = ['age', 'fare']\n","  \n","def one_hot_cat_column(feature_name, vocab):\n","  return tf.feature_column.indicator_column(\n","      tf.feature_column.categorical_column_with_vocabulary_list(feature_name,\n","                                                                vocab))\n","feature_columns = []\n","for feature_name in CATEGORICAL_COLUMNS:\n","  # Need to one-hot encode categorical features.\n","  vocabulary = train_feat[feature_name].unique()\n","  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n","  \n","for feature_name in NUMERIC_COLUMNS:\n","  feature_columns.append(tf.feature_column.numeric_column(feature_name,\n","                                                          dtype=tf.float32))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dfSbV6igl1EH"},"source":["###Input Function\n","\n","The Input function specifies how data is read into our model. We will use the from_tensor_slices method in the [tf.data](https://www.tensorflow.org/api_docs/python/tf/data) API to read in data directly from Pandas. This is suitable for smaller, in-memory datasets.\n","\n","We will need to create two input functions, one for our training data and one for the evaluation data.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xpKOoWgu-llD","colab":{}},"source":["# Use entire batch since this is such a small dataset.\n","NUM_EXAMPLES = len(train_labels)\n","\n","def make_input_fn(X, y, n_epochs=None, shuffle=True):\n","  def input_fn():\n","    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n","    if shuffle:\n","      dataset = dataset.shuffle(NUM_EXAMPLES)\n","    # For training, cycle thru dataset as many times as need (n_epochs=None).    \n","    dataset = dataset.repeat(n_epochs)\n","    # In memory training doesn't use batching.\n","    dataset = dataset.batch(NUM_EXAMPLES)\n","    return dataset\n","  return input_fn\n","\n","# Training and evaluation input functions.\n","train_input_fn = make_input_fn(train_feat, train_labels)\n","eval_input_fn = make_input_fn(eval_feat, eval_labels, shuffle=False, n_epochs=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4oS7LNqHpvYp","colab_type":"text"},"source":["##Training"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"L4EqVWg4-llM"},"source":["###Logistic Regression\n","\n","Before we build our boosted tree model, letâ€™s see how our model does using [ logistic regression](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html). Remember that logistic regression will give us a prediction if a passenger survives. \n","\n","Note:*If you see warning messages from the training of these estimators, do not be alarmed. The function will run as intended*\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Mr0GP-cQ-llN","colab":{}},"source":["linear_est = tf.estimator.LinearClassifier(feature_columns)\n","\n","# Train model.\n","linear_est.train(train_input_fn, max_steps=100)\n","\n","# Evaluation.\n","result = linear_est.evaluate(eval_input_fn)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Q0Q6xSyp6Jp","colab_type":"text"},"source":["Let's print our results from training:"]},{"cell_type":"code","metadata":{"id":"LE-0QNSvp9C1","colab_type":"code","colab":{}},"source":["print(\"Reslts from Linear Classifier: \\n\")\n","print(\"Accuracy: {}\".format(result['accuracy']*100))\n","print(\"Loss: {}\".format(result['loss']))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rOn0Orjvp_uL","colab_type":"text"},"source":["As you can see, using logistic regression our model was Okay. Let's see if we can improve the performance using Boosted Trees."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hCWYwkug-llQ"},"source":["###Boosted Trees\n","\n","Boosted Trees follow much of the same parameters, except you must define the number of batches. We'll use 1 batch because our dataset is small, and it can fit into memory"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-NpcXY9--llS","colab":{}},"source":["# Since data fits into memory, use entire dataset per layer. It will be faster.\n","# Above one batch is defined as the entire dataset. \n","n_batches = 1\n","est = tf.estimator.BoostedTreesClassifier(feature_columns,\n","                                          n_batches_per_layer=n_batches)\n","\n","# The model will stop training once the specified number of trees is built, not \n","# based on the number of steps.\n","est.train(train_input_fn, max_steps=100)\n","\n","# Eval.\n","result = est.evaluate(eval_input_fn)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pZpIf9HvqlDZ","colab_type":"text"},"source":["Again, let's print the results:"]},{"cell_type":"code","metadata":{"id":"ROvJ19HAqo0W","colab_type":"code","colab":{}},"source":["print(\"Reslts from Boosted Trees Classifier: \\n\")\n","print(\"Accuracy: {}\".format(result['accuracy']*100))\n","print(\"Loss: {}\".format(result['loss']))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fdypQcA9qv3R","colab_type":"text"},"source":["Much better! We can take things a step further by defining custom parameters for our boosted tree model."]},{"cell_type":"markdown","metadata":{"id":"3cFzp8f8rHnY","colab_type":"text"},"source":["###Custom Boosted Tree\n","\n","Let's explain some of the parameters that are modifiable for the boosted tree classifier:\n","* n_trees: Defines the number of trees to be created (more trees = more represented features)\n","* max_dpeth: maximum depth the tree can grow (larger depth = greater complexity)\n","* max_steps: number of actions the model can take to optimize itself\n","\n","Feel free to play around with these parameters to see if you can get your model to perform better.\n"]},{"cell_type":"code","metadata":{"id":"couLJMKRrDG1","colab_type":"code","colab":{}},"source":["params = {\n","  'n_trees': 50,\n","  'max_depth': 3,\n","  'n_batches_per_layer': 1,\n","  # You must enable center_bias = True to get DFCs. This will force the model to \n","  # make an initial prediction before using any features (e.g. use the mean of \n","  # the training labels for regression or log odds for classification when\n","  # using cross entropy loss).\n","  'center_bias': True\n","}\n","est = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\n","# Train model.\n","est.train(train_input_fn, max_steps=100)\n","# Evaluation.\n","result = est.evaluate(eval_input_fn)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"81-0oB--s4xU","colab_type":"text"},"source":["Then print your results: "]},{"cell_type":"code","metadata":{"id":"6BOkquE2s37X","colab_type":"code","colab":{}},"source":["print(\"Reslts from Custom Boosted Trees Classifier: \\n\")\n","print(\"Accuracy: {}\".format(result['accuracy']*100))\n","print(\"Loss: {}\".format(result['loss']))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9EEGuDVuzb5r"},"source":["## Evaluate the model\n","\n","Random forests can be hard to evaluate as captured in this [paper](http://blog.datadive.net/interpreting-random-forests/): *Most literature on random forests and interpretable models would lead you to believe this is nigh impossible, since random forests are typically treated as a black box. Indeed, a forest consists of a large number of deep trees, where each tree is trained on bagged data using random selection of features, so gaining a full understanding of the decision process by examining each individual tree is infeasible. Furthermore, even if we are to examine just a single tree, it is only feasible in the case where it has a small depth and low number of features. A tree of depth 10 can already have thousands of nodes, meaning that using it as an explanatory model is almost impossible.*\n","\n","To get around this, we will use a technique called Directional Feature Contributions (DFCs). DFCs are a way to compute the importance our model gives to each feature. Our model has a built in function called [experimental_feature_importances](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier#experimental_feature_importances) that we can use to graph the feature importance.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zOMKywn4zReN","colab":{}},"source":["# Get importances\n","importances = est.experimental_feature_importances(normalize=True)\n","df_imp = pd.Series(importances)\n","\n","# Visualize importances.\n","N = 9\n","ax = (df_imp.iloc[0:N][::-1]\n","    .plot(kind='barh'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z1iEXVTR0Z2t"},"source":["By analyzing the DFCs, we can see that our model thinks that sex and your ticket far are the most powerful measures of your survival. \n","\n","An interesting experiment is to go back and change how many trees and the depth of your random forest. See if it affects the DFCs.\n"]}]}